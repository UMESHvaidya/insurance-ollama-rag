# ğŸ¥ Insurance Policy RAG System

A **100% Free & Local** Retrieval-Augmented Generation (RAG) system for analyzing insurance policy documents using **Ollama** and local embeddings.

## âœ¨ Features

- ğŸ†“ **Completely Free**: No API keys or cloud services required
- ğŸ”’ **Privacy First**: All processing happens locally on your machine
- ğŸ“„ **PDF Processing**: Intelligent document loading with context preservation
- ğŸ” **Smart Retrieval**: Vector-based similarity search with ChromaDB
- ğŸ¤– **Local LLM**: Powered by Ollama (gemma2:2b, llama2, mistral, etc.)
- ğŸ¨ **Beautiful CLI**: Rich terminal interface with colors and spinners
- âš™ï¸ **Configurable**: Easy configuration via .env file

## ğŸš€ Quick Start

### Prerequisites

1. **Python 3.10+**
2. **Poetry** - Python dependency manager
3. **Ollama** - Local LLM runtime

### Step 1: Install Ollama

```bash
# macOS/Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows
# Download from https://ollama.com/download

# Start Ollama service
ollama serve

# Pull the model (in a new terminal)
ollama pull gemma2:2b  # Lightweight and fast (2GB)
# or
ollama pull llama2     # More capable (4GB)
# or
ollama pull mistral    # Good balance (4GB)
```

### Step 2: Install Poetry

```bash
# macOS/Linux
curl -sSL https://install.python-poetry.org | python3 -

# Windows (PowerShell)
(Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | py -

# Verify installation
poetry --version
```

### Step 3: Setup Project

```bash
# Clone/create project directory
mkdir insurance-policy-rag
cd insurance-policy-rag

# Copy all the code files (see structure below)

# Install dependencies
poetry install

# Create .env file
cp .env.example .env

# Edit .env if needed (optional, defaults work fine)
nano .env
```

### Step 4: Add Your Policy Document

```bash
# Create data directory
mkdir -p data

# Copy your insurance policy PDF
cp /path/to/your/policy.pdf data/insurance_policy.pdf
```

### Step 5: Run!

```bash
# Run example queries
poetry run python main.py

# Or use interactive mode
poetry run python scripts/interactive.py
```

## ğŸ“ Project Structure

```
insurance-policy-rag/
â”œâ”€â”€ pyproject.toml              # Poetry configuration
â”œâ”€â”€ poetry.lock                 # Locked dependencies
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ .env.example               # Environment template
â”œâ”€â”€ .gitignore                 # Git ignore
â”œâ”€â”€ data/                      # Policy documents
â”‚   â””â”€â”€ insurance_policy.pdf
â”œâ”€â”€ src/insurance_rag/         # Main package
â”‚   â”œâ”€â”€ __init__.py           # Package exports
â”‚   â”œâ”€â”€ config.py             # Settings (Ollama config)
â”‚   â”œâ”€â”€ models.py             # Data models
â”‚   â”œâ”€â”€ document_loader.py    # PDF processing
â”‚   â”œâ”€â”€ vectorstore.py        # Vector DB (local embeddings)
â”‚   â”œâ”€â”€ retriever.py          # Document retrieval
â”‚   â”œâ”€â”€ llm_analyzer.py       # Ollama integration
â”‚   â””â”€â”€ rag_pipeline.py       # Main orchestrator
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ interactive.py        # Interactive CLI
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_rag.py          # Tests
â””â”€â”€ main.py                   # Entry point
```

## âš™ï¸ Configuration

Configure via `.env` file:

```bash
# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=gemma2:2b

# Embedding Model (HuggingFace, runs locally)
EMBEDDING_MODEL=all-MiniLM-L6-v2

# RAG Parameters (optimized for smaller models)
CHUNK_SIZE=800
CHUNK_OVERLAP=150
RETRIEVAL_K=4
LLM_TEMPERATURE=0.1
CONTEXT_LENGTH=2048

# Directories
DATA_DIR=data
```

## ğŸ¯ Model Recommendations

| Model         | Size | RAM Needed | Speed  | Quality    | Best For                     |
| ------------- | ---- | ---------- | ------ | ---------- | ---------------------------- |
| **gemma2:2b** | 2GB  | 4GB        | âš¡âš¡âš¡ | â­â­â­     | Fast queries, basic analysis |
| **llama2**    | 4GB  | 8GB        | âš¡âš¡   | â­â­â­â­   | Better understanding         |
| **mistral**   | 4GB  | 8GB        | âš¡âš¡   | â­â­â­â­   | Balanced performance         |
| **llama3:8b** | 8GB  | 16GB       | âš¡     | â­â­â­â­â­ | Best quality                 |

Switch models anytime:

```bash
# Pull new model
ollama pull llama2

# Update .env
OLLAMA_MODEL=llama2

# Restart the app
poetry run python main.py
```

## ğŸ“Š Example Output

```
â“ QUERY: Is cataract surgery covered?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Covered
Cataract surgery is explicitly listed as a covered procedure under
the Day Care Procedures section of the policy document.
Reference: Page 6, Section 3.2 - Day Care Procedures
Confidence: High
```

## ğŸ¯ Coverage Status Types

- **âœ… Covered**: Explicitly mentioned as covered
- **âŒ Not Covered**: Explicitly excluded or not mentioned
- **âš ï¸ Ambiguous**: Unclear, conditional, or needs clarification

## ğŸ’» Usage Examples

### Programmatic Usage

```python
from insurance_rag import InsurancePolicyRAG

# Initialize
rag = InsurancePolicyRAG()

# Load policy
rag.load_policy("data/insurance_policy.pdf")

# Query
response = rag.query("Is dental treatment covered?")
print(response)

# Access response fields
print(f"Status: {response.status}")
print(f"Explanation: {response.explanation}")
print(f"Reference: {response.reference}")
print(f"Confidence: {response.confidence}")

# Convert to dict
data = response.to_dict()
```

### Interactive Mode Features

- ğŸ¨ Colored output based on coverage status
- âš¡ Loading spinners for better UX
- ğŸ’¬ Natural conversation flow
- ğŸ”„ Keep asking multiple questions
- ğŸ“‹ Clear, formatted responses

## ğŸ§ª Testing

```bash
# Run all tests
poetry run pytest

# With coverage
poetry run pytest --cov=insurance_rag

# Verbose output
poetry run pytest -v
```

## ğŸ› ï¸ Development

```bash
# Format code
poetry run black src/ scripts/ tests/

# Lint
poetry run ruff check src/ scripts/ tests/

# Type checking
poetry run mypy src/

# Add new dependency
poetry add <package-name>

# Add dev dependency
poetry add --group dev <package-name>
```

## ğŸ”§ Troubleshooting

### Issue: "Cannot connect to Ollama"

```bash
# Make sure Ollama is running
ollama serve

# Check if it's running
curl http://localhost:11434/api/tags
```

### Issue: "Model not found"

```bash
# Pull the model
ollama pull gemma2:2b

# List available models
ollama list
```

### Issue: "Out of memory"

```bash
# Use a smaller model
ollama pull gemma2:2b  # Only 2GB

# Or reduce context length in .env
CONTEXT_LENGTH=1024
CHUNK_SIZE=500
```

### Issue: "Slow performance"

```bash
# Use smaller chunks
CHUNK_SIZE=600
RETRIEVAL_K=3

# Or use a smaller/faster model
OLLAMA_MODEL=gemma2:2b
```

### Issue: "Poetry command not found"

```bash
# Add Poetry to PATH
export PATH="$HOME/.local/bin:$PATH"

# Or use full path
~/.local/bin/poetry install
```

## ğŸš€ Performance Tips

1. **GPU Acceleration**: If you have NVIDIA GPU:

   ```python
   # In vectorstore.py, change:
   model_kwargs={'device': 'cuda'}  # Instead of 'cpu'
   ```

2. **Faster Embeddings**: Use smaller embedding model:

   ```bash
   EMBEDDING_MODEL=all-MiniLM-L6-v2  # Fast, 80MB
   ```

3. **Persistent Vector Store**: Cache embeddings:
   ```python
   # Modify vectorstore.py to persist to disk
   persist_directory="./chroma_db"
   ```

## ğŸ“ Customization Examples

### Use Different PDF Location

```python
rag = InsurancePolicyRAG()
rag.load_policy("/custom/path/policy.pdf")
```

### Analyze Multiple Policies

```python
rag = InsurancePolicyRAG()

# Policy 1
rag.load_policy("data/health_policy.pdf")
response1 = rag.query("Is surgery covered?")

# Policy 2
rag.load_policy("data/life_policy.pdf")
response2 = rag.query("What is the coverage amount?")
```

### Custom Prompts

Edit `src/insurance_rag/llm_analyzer.py` to modify the prompt template.

## ğŸŒŸ Why Local LLMs?

### Advantages

- âœ… **No API Costs**: Completely free forever
- âœ… **Privacy**: Your data never leaves your machine
- âœ… **No Rate Limits**: Query as much as you want
- âœ… **Offline**: Works without internet (after initial model download)
- âœ… **Customizable**: Full control over models and prompts

### Considerations

- âš ï¸ **Setup Required**: Need to install and run Ollama
- âš ï¸ **Hardware**: Needs decent RAM (4-16GB depending on model)
- âš ï¸ **Speed**: Slower than cloud APIs (but still fast enough)
- âš ï¸ **Quality**: Smaller models less capable than GPT-4

## ğŸ”„ Migration from OpenAI

If you have an existing OpenAI-based RAG system:

```python
# Old (OpenAI)
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

llm = ChatOpenAI(api_key="sk-...", model="gpt-4")
embeddings = OpenAIEmbeddings(api_key="sk-...")

# New (Local)
import ollama
from langchain_community.embeddings import HuggingFaceEmbeddings

# Use ollama.generate() for LLM
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
```

## ğŸ“š Additional Resources

- [Ollama Documentation](https://github.com/ollama/ollama)
- [Available Models](https://ollama.com/library)
- [LangChain Docs](https://python.langchain.com/docs/get_started/introduction)
- [ChromaDB Guide](https://docs.trychroma.com/)

## ğŸ¤ Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

MIT License - See LICENSE file for details

## ğŸ’¬ Support

- **Issues**: Open a GitHub issue
- **Questions**: Discussions tab
- **Updates**: Watch the repository

## ğŸ‰ What's Next?

Possible enhancements:

- [ ] Web UI with Streamlit/Gradio
- [ ] Multi-document comparison
- [ ] Export reports to PDF
- [ ] Document question history
- [ ] Fine-tune models on insurance documents
- [ ] Add voice input/output
- [ ] Mobile app

---

Made with â¤ï¸ using local AI. No clouds were harmed in the making of this project! â˜ï¸ğŸš«